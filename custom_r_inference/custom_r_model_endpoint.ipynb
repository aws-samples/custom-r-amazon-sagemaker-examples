{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a662cd-f2f7-4aa1-9af6-5e03a5d3aeea",
   "metadata": {},
   "source": [
    "# Deploying a custom R model behind a single model endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282c8ee-93e1-4613-b198-51684955de0c",
   "metadata": {},
   "source": [
    "This notebook runs through the steps to deploy a custom R model behind a single model SageMaker endpoint for online inference. \n",
    "\n",
    "The same custom container can be used for both training and inference, however this example covers an inference only container. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95e54f-f9ac-472e-9d97-7086a8ff72e7",
   "metadata": {},
   "source": [
    "## Inference container requirements & process\n",
    "\n",
    "* For inference, SageMaker runs container as ```docker run image serve```. So we can use the serve argument to differentiate between training and inference if we have both in the same container\n",
    "* For the inference container, we must have created a model which includes specifying the S3 location of the trained model, which must point to tar.gz file. This gets loaded into the /opt/ml/model directorty in the container\n",
    "* The container serves requests by implementing /invocations and /ping endpoints on port 8080\n",
    "* /ping should respond with 200 status code and empty body. This signals container is ready to accept inference requests. \n",
    "\n",
    "See more here - https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bdd3b9-a60a-4d35-a429-ee861457581f",
   "metadata": {},
   "source": [
    "## Deployment Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951ba1f-79cb-47fb-a428-4db498eb302d",
   "metadata": {},
   "source": [
    "### 1. Build the custom R model image\n",
    "\n",
    "First, we define the Docker image that implements the required endpoints and directory structure. \n",
    "\n",
    "In this example, the custom image uses an R based xgb model to predict species based on the Iris dataset. \n",
    "\n",
    "In the r_xgb_iris_image directory:\n",
    "* Dockerfile defines the image\n",
    "* deploy.R includes the logic to load and run data through the model\n",
    "* endpoints.R implements the web server endpoints we need to provide for use with SageMaker. This uses the R plumber package. \n",
    "* build_and_push_docker.sh is a helper script to aid in publishing the image to Amazon Elastic Container Registry\n",
    "* xgb.model is the pretrained model that we tar.gz and upload to S3 and then use to create a model. We load the model into the container in this example so that we can test the container locally but this is not required. When deployed with SageMaker, the model is loaded from S3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b3d97-4eed-41d1-a9ce-3f19b3fbdd6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker build -t r-iris-inference ./r_xgb_iris_image/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde6aa2-5c02-4787-afa8-fb585c325154",
   "metadata": {},
   "source": [
    "### 2. Launch the inference container\n",
    "\n",
    "Next, we can test the container by running it locally. Here we map the local port 5000 to the container port 8080 that is serving our endpoints. \n",
    "\n",
    "We pass the serve argument to simulate how SageMaker runs the image for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e4d5f-454e-49e0-89c0-2c821522ae80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run -d --rm -p 5000:8080 r-iris-inference:latest serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4231280-84da-4b07-8cea-da1627f32df3",
   "metadata": {},
   "source": [
    "View the running container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a860d31-42e1-4ecc-ba39-d1f7bc0208d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker container list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db78695-c075-43d1-8d80-5a6d86bb9089",
   "metadata": {},
   "source": [
    "You can stop the container when finished testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef74df-7dc9-45c9-936c-74df2b36dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop brave_bassi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b5098-f053-4359-9f3c-fbf433f72137",
   "metadata": {},
   "source": [
    "### 3. Load sample data\n",
    "\n",
    "We load the iris dataset and extract just the features. We then convert to a list of features as this is how our invocation endpoint expects to receive data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b94dd3-b0a1-4ab5-bbd3-f7bc98d658a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650d23f-0726-47c1-80b7-926915503628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_names = [\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\", \"Label\"]\n",
    "iris = pd.read_csv(\n",
    "    \"s3://sagemaker-sample-files/datasets/tabular/iris/iris.data\", names=column_names\n",
    ")\n",
    "iris_features = iris[[\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\"]]\n",
    "example_inputs = iris_features.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ba46d-0698-4383-be1d-8530025df4be",
   "metadata": {},
   "source": [
    "### 4. Test the local inference container\n",
    "\n",
    "We run two tests:\n",
    "1. Invoke /ping endpoint to check for HTTP 200 response\n",
    "2. Invoke /invocations endpoint to check for predicted class response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac469c8-f520-4626-9577-39513424585b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4681f02e-2270-45ee-bef0-d4c1611ae3be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "requests.get('http://localhost:5000/ping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb31a59-d1e8-4a12-8cc2-b7f4ee347c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the key \"features\" is important since we look for this value in our inference container\n",
    "payload = {\"features\": example_inputs}\n",
    "response = requests.post(f\"http://localhost:5000/invocations\", json=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a7d4f-064c-4ba8-b185-7874437adbe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f398d-30c3-4914-b3df-b2ca17159c08",
   "metadata": {},
   "source": [
    "### 5. Deploy the inference container behind a SageMaker endpoint\n",
    "\n",
    "Now we have confirmed locally that the inference container behaves as expected, we can deploy it behind a SageMaker endpoint. \n",
    "This involves several steps:\n",
    "1. Deploy the custom inference image to ECR repo\n",
    "2. tar and zip model artifacts (in this case xgb.model) and upload to S3\n",
    "3. Create a SageMaker model that references the trained model artifact in S3\n",
    "4. Create an endpoint configuration \n",
    "5. Create the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db6cf04-1f59-46b9-916f-6ad5e6e64039",
   "metadata": {},
   "source": [
    "#### Deploy to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f59164-500e-4c2a-b7c5-b43cf9835674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deploy the custom inference image to ECR. this can be done using the helper script, note ecr permissions are required \n",
    "# pass image name and tag as arguments \n",
    "! ./r_xgb_iris_image/build_and_push_docker.sh r-iris-inference latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9586fb-162e-4c78-b7da-fdb7949cbaaf",
   "metadata": {},
   "source": [
    "#### Upload model artifacts to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80943524-38e9-4a28-a985-edd05d3dcd27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload artifacts to s3\n",
    "! cd r_xgb_iris_image && tar czf xgb.tar.gz xgb.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedef853-7c32-4419-bc36-b54509c0a501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket_name = \"<bucket-name>\"\n",
    "bucket_key = \"models/iris/xgb.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0278dc39-47b4-4cb6-9a62-2392d295e541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file('r_xgb_iris_image/xgb.tar.gz', bucket_name, bucket_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e8800-d06d-4e09-b3aa-c516ffa8d72c",
   "metadata": {},
   "source": [
    "#### Create the SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d2e0b-146d-481c-804e-b3f299b6c270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import Session\n",
    "\n",
    "aws_region = boto3.Session().region_name\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f6938-3b08-43e5-8150-3a41184b9e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\", region_name=aws_region)\n",
    "\n",
    "model_url = f\"s3://{bucket_name}/{bucket_key}\"\n",
    "\n",
    "image_uri = f\"{account_id}.dkr.ecr.{aws_region}.amazonaws.com/r-iris-inference:latest\"\n",
    "container = {\"Image\": image_uri, \"ModelDataUrl\": model_url, \"Mode\": \"SingleModel\"}\n",
    "model_name = \"r-iris-inference\"\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name, \n",
    "    ExecutionRoleArn=sagemaker_role, \n",
    "    PrimaryContainer=container\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecef140-ebeb-49d9-a100-1c0a080a62d3",
   "metadata": {},
   "source": [
    "#### Create an endpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c0a6d1-019b-4eab-b925-f11c9d239ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = \"r-iris-inference-endpoint-config\"\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        \"InstanceType\": \"ml.m4.xlarge\", # the best instance type will depend on your use case and model\n",
    "        \"InitialInstanceCount\": 1,\n",
    "        \"ModelName\": model_name,\n",
    "        \"VariantName\": \"AllTraffic\",\n",
    "    }],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4525a1-ec8c-4d91-a175-73b0d81f6598",
   "metadata": {},
   "source": [
    "#### Create endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e8b51-a449-48e0-ab21-feb3d38f2caf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"r-iris-endpoint\"\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "# create our waiter to let us know when the endpoint is in service\n",
    "print(\"Waiting for {} endpoint to be in service...\".format(endpoint_name))\n",
    "waiter = sm_client.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7fd72-dac1-4617-8ad6-62be1c8f23a2",
   "metadata": {},
   "source": [
    "### 6. Test the endpoint\n",
    "\n",
    "Once the endpoint has come into service, we can send some of our sample data for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39b6e6-2dd9-46c2-8007-7dabceb10eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "payload = {\"features\": example_inputs}\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "    print(\"Healthy\")\n",
    "else: \n",
    "    print(\"Not healthy\")\n",
    "    \n",
    "print(\"Response: \", json.loads(response['Body'].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e0f82-961f-4f7a-8e2f-aef21209037c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
